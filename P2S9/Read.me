T3D Psuedo Code-

![Github Logo](F:\EVA\EVA\Phase - 2\Session 9\T3D-PsuedoCode.png)

<###> STEP 1 
We initialize the Experience Replay Memory with a size of 1e6. Then we populate it with new transitions
(This is a standard replay buffer borrowed from the OpenAI baseline repo)

### STEP 2 (Build one DNN for the Actor model and one for Actor Target)
This is a fairly standard set up for both Actor and Critic networks. Note that the critic class actually contains both networks to be used. The critics forward() method returns the Q values for both critics to be used later.

### STEP 3 (Build two DNNs for the two Critic models and two DNNs for the two Critic Targets)
The get_Q method simply returns the first critic network.

### STEP 4 (Sample from a batch of transitions (s, s', a, r) from the memory)
Once we have carried out a full time step through the environment, we train our model for several iterations. The first step in the update carried it involves the critic. This is one of the most important parts of the algorithm and where most of the TD3 additional features are implemented. First thing to do is to sample a mini batch of stored transitions from the replay buffer.

### STEP 5 The actor target plays the next action a', From the next state s'.


### STEP 6 (We add Gaussian noise to this next action a' and we clamp it in a range of values supported by the environment) 
Next we are going to select an action for each of the states that we have pulled in from our mini batch and apply the target policy smoothing. As described earlier, this is just picking an action with our target actor network and we add noise to that action that has been clipped in order to ensure that the noisy action isn’t too far away from the original action value.

### STEP 7 (The two Critic targets take each the couple (s', a') as input and return two Q values, Qt1(s', a') and Qt2(s', a') as outputs)
Next we need to compute our target Q values of the critic. This is where the double critic networks come into play. 

### STEP 8 (Keep the minimum of these two Q-Values)
We are going to get the Q values for each target critic and then take the smallest of the two for our target Q value.

### STEP 9 (We get the final target of the two Critic models, which is: Qt = r + gamma * min(Qt1, Qt2))
We are going to get the Q values for each target critic and then take the smallest of the two for our target Q value.

### STEP 10 (Two critic models take (s, a) and return two Q-Vales)
Finally we calculate the loss for the two current critic networks. 

### STEP 11 (Compute the Critic Loss)
This is done by getting the MSE of each current critic and the target Q value we just calculated. We then carry out the optimisation of the critic as normal.

### STEP 12 (Backpropagate this critic loss and update the parameters of two Critic models)
This is done by getting the MSE of each current critic and the target Q value we just calculated. We then carry out the optimisation of the critic as normal.

### STEP 13 (Once every two iterations, we update our Actor model by performing gradient ASCENT on the output of the first Critic model)
The actor is much simpler to update when compared to the critic. First we make sure that we are only updating the actor every d time steps. In our case and in the paper, the actor was updated every 2nd time step. The actor’s loss function simply gets the mean of the -Q values from our critic network with our actor choosing what action to take given the mini batch of states. Just like before, we optimise our actor network through backpropagation.

### STEP 14 (Still, in once every two iterations, we update our "Actor Target" by Polyak Averaging)
Finally we update our frozen target networks using a soft update. This is done along side the "Actor" update and is also delayed.

### STEP 15 (Still, in once every two iterations, we update our "Critic Target" by Polyak Averaging)
Finally we update our frozen target networks using a soft update. This is done along side the "Critic" update and is also delayed.

